{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_x: Tensor(\"input_x:0\", shape=(128, 50), dtype=int32)\n",
      "get_mask==>result: Tensor(\"mul:0\", shape=(50, 50), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from a2_base_model import BaseClass\n",
    "from a2_encoder import Encoder\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s:%(levelname)s:%(message)s',level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN=\"article\"\n",
    "MAX_SEQ_LEN=1500\n",
    "train_X=np.load(f'../input/{TOKEN}_train_X.npy')\n",
    "train_y=np.load(f'../input/{TOKEN}_train_y.npy')\n",
    "train_len=np.load(f'../input/{TOKEN}_train_len.npy')\n",
    "dev_X=np.load(f'../input/{TOKEN}_dev_X.npy')\n",
    "dev_y=np.load(f'../input/{TOKEN}_dev_y.npy')\n",
    "dev_len=np.load(f'../input/{TOKEN}_dev_len.npy')\n",
    "embedding=np.load(f'../input/{TOKEN}_embedding.npy')\n",
    "valid_char=np.load(f'../input/{TOKEN}_valid_char.npy')\n",
    "labels=[]\n",
    "for i in range(1,20):\n",
    "    labels.append(str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(BaseClass):\n",
    "    def __init__(self, num_classes, learning_rate, batch_size, decay_steps, decay_rate, sequence_length,\n",
    "                 vocab_size,embedding_mat,embed_size,d_model,d_k,d_v,h,num_layer,is_training,\n",
    "                 initializer=tf.random_normal_initializer(stddev=0.1),clip_gradients=5.0,l2_lambda=0.0001,use_residual_conn=False):\n",
    "        \"\"\"init all hyperparameter here\"\"\"\n",
    "        super(Transformer, self).__init__(d_model, d_k, d_v, sequence_length, h, batch_size, num_layer=num_layer) #init some fields by using parent class.\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = d_model\n",
    "        self.learning_rate = tf.Variable(learning_rate, trainable=False, name=\"learning_rate\")\n",
    "        self.learning_rate_decay_half_op = tf.assign(self.learning_rate, self.learning_rate * 0.5)\n",
    "        self.initializer = initializer\n",
    "        self.clip_gradients=clip_gradients\n",
    "        self.l2_lambda=l2_lambda\n",
    "\n",
    "        self.is_training=is_training #self.is_training=tf.placeholder(tf.bool,name=\"is_training\") #tf.bool #is_training\n",
    "        self.input_x = tf.placeholder(tf.int32, [self.batch_size, self.sequence_length], name=\"input_x\")                 #x  batch_size\n",
    "        self.input_y_label = tf.placeholder(tf.int32, [self.batch_size,num_classes], name=\"input_y_label\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        self.global_step = tf.Variable(0, trainable=False, name=\"Global_Step\")\n",
    "        self.epoch_step = tf.Variable(0, trainable=False, name=\"Epoch_Step\")\n",
    "        self.epoch_increment = tf.assign(self.epoch_step, tf.add(self.epoch_step, tf.constant(1)))\n",
    "        self.decay_steps, self.decay_rate = decay_steps, decay_rate\n",
    "        self.use_residual_conn=use_residual_conn\n",
    "\n",
    "        self.instantiate_weights(embedding_mat)\n",
    "        self.logits = self.inference() #logits shape:[batch_size,self.num_classes]\n",
    "\n",
    "        self.predictions = tf.argmax(self.logits, axis=1, name=\"predictions\")\n",
    "        self.pred=tf.cast(self.predictions, tf.int32)\n",
    "        self.true=tf.cast(tf.argmax(self.input_y_label,1),tf.int32)\n",
    "        correct_prediction = tf.equal(self.pred,self.true)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name=\"Accuracy\")  # shape=()\n",
    "        if self.is_training is False:# if it is not training, then no need to calculate loss and back-propagation.\n",
    "            return\n",
    "        print('logits',self.logits.shape)\n",
    "        print('labels',self.input_y_label.shape)\n",
    "        self.loss_val = self.loss()\n",
    "        self.train_op = self.train()\n",
    "\n",
    "    def inference(self):\n",
    "        \"\"\" building blocks:\n",
    "        encoder:6 layers.each layers has two   sub-layers. the first is multi-head self-attention mechanism; the second is position-wise fully connected feed-forward network.\n",
    "               for each sublayer. use LayerNorm(x+Sublayer(x)). all dimension=512.\n",
    "        decoder:6 layers.each layers has three sub-layers. the second layer is performs multi-head attention over the ouput of the encoder stack.\n",
    "               for each sublayer. use LayerNorm(x+Sublayer(x)).\n",
    "        \"\"\"\n",
    "        # 1.embedding for encoder input & decoder input\n",
    "        # 1.1 position embedding for encoder input\n",
    "        input_x_embeded = tf.nn.embedding_lookup(self.Embedding,self.input_x)  #[None,sequence_length, embed_size]\n",
    "        input_x_embeded=tf.multiply(input_x_embeded,tf.sqrt(tf.cast(self.d_model,dtype=tf.float32)))\n",
    "        input_mask=tf.get_variable(\"input_mask\",[self.sequence_length,1],initializer=self.initializer)\n",
    "        input_x_embeded=tf.add(input_x_embeded,input_mask) #[None,sequence_length,embed_size].position embedding.\n",
    "\n",
    "        # 2. encoder\n",
    "        encoder_class=Encoder(self.d_model,self.d_k,self.d_v,self.sequence_length,self.h,self.batch_size,self.num_layer,input_x_embeded,input_x_embeded,dropout_keep_prob=self.dropout_keep_prob,use_residual_conn=self.use_residual_conn)\n",
    "        Q_encoded,K_encoded = encoder_class.encoder_fn() #K_v_encoder\n",
    "\n",
    "        Q_encoded=tf.reshape(Q_encoded,shape=(self.batch_size,-1)) #[batch_size,sequence_length*d_model]\n",
    "        with tf.variable_scope(\"output\"):\n",
    "            logits = tf.matmul(Q_encoded, self.W_projection) + self.b_projection #logits shape:[batch_size*decoder_sent_length,self.num_classes]\n",
    "        #print(\"logits:\",logits)\n",
    "        return logits\n",
    "\n",
    "    def loss(self, l2_lambda=0.0001):  # 0.001\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            # input: `logits`:[batch_size, num_classes], and `labels`:[batch_size]\n",
    "            # output: A 1-D `Tensor` of length `batch_size` of the same type as `logits` with the softmax cross entropy loss.\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(labels=self.input_y_label,logits=self.logits);  # sigmoid_cross_entropy_with_logits.#losses=tf.nn.softmax_cross_entropy_with_logits(labels=self.input_y,logits=self.logits)\n",
    "            # print(\"1.sparse_softmax_cross_entropy_with_logits.losses:\",losses) # shape=(?,)\n",
    "            loss = tf.reduce_mean(losses)  # print(\"2.loss.loss:\", loss) #shape=()\n",
    "            l2_losses = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if ('bias' not in v.name ) and ('alpha' not in v.name)]) * l2_lambda\n",
    "            loss = loss + l2_losses\n",
    "        return loss\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"based on the loss, use SGD to update parameter\"\"\"\n",
    "        learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step, self.decay_steps,self.decay_rate, staircase=True)\n",
    "        self.learning_rate_=learning_rate\n",
    "        #noise_std_dev = tf.constant(0.3) / (tf.sqrt(tf.cast(tf.constant(1) + self.global_step, tf.float32))) #gradient_noise_scale=noise_std_dev\n",
    "        train_op = tf.contrib.layers.optimize_loss(self.loss_val, global_step=self.global_step,\n",
    "                                                   learning_rate=learning_rate, optimizer=\"Adam\",clip_gradients=self.clip_gradients)\n",
    "        return train_op\n",
    "\n",
    "    def instantiate_weights(self,embedding_mat):\n",
    "        \"\"\"define all weights here\"\"\"\n",
    "        with tf.variable_scope(\"embedding_projection\"):  # embedding matrix\n",
    "            self.Embedding = tf.get_variable(\"Embedding\", shape=[self.vocab_size, self.embed_size],initializer=self.initializer)\n",
    "            #self.Embedding = tf.Variable(embedding_mat+1,name='dynamic_W',dtype=tf.float32)\n",
    "            #self.Embedding_label = tf.get_variable(\"Embedding_label\", shape=[self.num_classes, self.embed_size],dtype=tf.float32) #,initializer=self.initializer\n",
    "            self.W_projection = tf.get_variable(\"W_projection\", shape=[self.sequence_length*self.d_model, self.num_classes],initializer=self.initializer)  # [embed_size,label_size]\n",
    "            self.b_projection = tf.get_variable(\"b_projection\", shape=[self.num_classes])\n",
    "\n",
    "    def get_mask(self,sequence_length):\n",
    "        lower_triangle = tf.matrix_band_part(tf.ones([sequence_length, sequence_length]), -1, 0)\n",
    "        result = -1e9 * (1.0 - lower_triangle)\n",
    "        print(\"get_mask==>result:\", result)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(X,y,batch_size,num_epochs,shuffle=True):\n",
    "    data_size=len(X)\n",
    "    num_batches_per_epoch=int((data_size-1)/batch_size)+1\n",
    "    for epoch in range(num_epochs):\n",
    "        # shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices=np.random.permutation(np.arange(data_size))\n",
    "            X=X[shuffle_indices]\n",
    "            y=y[shuffle_indices]\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index=batch_num*batch_size\n",
    "            end_index=(batch_num+1)*batch_size\n",
    "            if end_index>data_size:\n",
    "                continue\n",
    "            yield X[start_index:end_index],y[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(rain_X,train_y,dev_X,dev_y,batch_size,epoch):\n",
    "    with tf.Graph().as_default():\n",
    "        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\n",
    "        sess=tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "        with sess.as_default():\n",
    "            model=Transformer(num_classes=19,\n",
    "                              learning_rate=0.01,\n",
    "                              batch_size=64,\n",
    "                              decay_steps=int(len(train_X)/batch_size),\n",
    "                              decay_rate=0.9,\n",
    "                              sequence_length=MAX_SEQ_LEN,\n",
    "                             vocab_size=len(valid_char),\n",
    "                              embedding_mat=embedding,\n",
    "                              embed_size=512,\n",
    "                              d_model=512,d_k=64,d_v=64,h=8,\n",
    "                              num_layer=2,is_training=True,\n",
    "                              use_residual_conn=True)\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            def train_step(x_batch,y_batch):\n",
    "                feed_dict={\n",
    "                    model.input_x:x_batch,\n",
    "                    model.input_y_label:y_batch,\n",
    "                    model.dropout_keep_prob:0.8\n",
    "                }\n",
    "                _,step,loss=sess.run(\n",
    "                    [model.train_op,model.global_step,model.loss_val],feed_dict)\n",
    "            def dev_step(x_batch,y_batch):\n",
    "                feed_dict={\n",
    "                    model.input_x:x_batch,\n",
    "                    model.input_y_label:y_batch,\n",
    "                    model.dropout_keep_prob:1.0\n",
    "                }\n",
    "                step,loss,y_pred,y_true=sess.run(\n",
    "                    [model.global_step,model.loss_val,model.pred,model.true],feed_dict)\n",
    "                #logger.info(\"step{},loss {:g}  acc:{}\".format(step,loss,acc))\n",
    "                return y_pred,y_true\n",
    "\n",
    "            batches=batch_iter(\n",
    "                train_X,train_y,batch_size,epoch,shuffle=True)\n",
    "            num_batches_per_epoch=int((len(train_X)-1)/batch_size)+1\n",
    "            num_epoch=0\n",
    "            for (x_batch,y_batch) in batches:\n",
    "                train_step(x_batch,y_batch)\n",
    "                current_step=tf.train.global_step(sess,model.global_step)\n",
    "                #saver.save(sess,checkpoint_prefix,global_step=current_step)\n",
    "                if current_step%num_batches_per_epoch==0:\n",
    "                    num_epoch+=1\n",
    "                    dev_batches=batch_iter(dev_X,dev_y,64,1,False)\n",
    "                    pred=[]\n",
    "                    true=[]\n",
    "                    for (x_bat,y_bat) in dev_batches:\n",
    "                        y_pred,y_true=dev_step(x_bat,y_bat)\n",
    "                        pred+=y_pred.tolist()\n",
    "                        true+=y_true.tolist()\n",
    "                    f1=f1_score(true,pred,average='weighted')\n",
    "                    acc=accuracy_score(true,pred)\n",
    "                    logger.info(f'{num_epoch}epoch: f1:{f1}   acc:{acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_fn.started.\n",
      "MultiHeadAttention.self.dropout_rate: Tensor(\"base_mode_sub_layer_multi_head_attention_encoder0/sub:0\", dtype=float32)\n",
      "scaled_dot_product_attention_batch.===============================================================>mask is not none? False\n",
      "dot_product:====================================================================================> Tensor(\"base_mode_sub_layer_multi_head_attention_encoder0/MatMul_1:0\", shape=(64, 8, 1500, 64), dtype=float32)\n",
      "self.sequence_length: 1500\n",
      "@@@========================>layer_input: Tensor(\"Add_1:0\", shape=(64, 1500, 512), dtype=float32) ;layer_output: Tensor(\"base_mode_sub_layer_multi_head_attention_encoder0/dense_3/BiasAdd:0\", shape=(64, 1500, 512), dtype=float32)\n",
      "LayerNormResidualConnection.use_residual_conn: True\n",
      "layer_normalization:==================>variable_scope: layer_normalization0encoder_multi_head_attention\n",
      "output_conv1: Tensor(\"sub_layer_postion_wise_feed_forwardencoder0/conv1:0\", shape=(64, 1500, 1, 1), dtype=float32)\n",
      "@@@========================>layer_input: Tensor(\"layer_normalization0encoder_multi_head_attention/add_1:0\", shape=(64, 1500, 512), dtype=float32) ;layer_output: Tensor(\"sub_layer_postion_wise_feed_forwardencoder0/Squeeze:0\", shape=(64, 1500, 512), dtype=float32)\n",
      "LayerNormResidualConnection.use_residual_conn: True\n",
      "layer_normalization:==================>variable_scope: layer_normalization0encoder_postion_wise_ff\n",
      "encoder_fn. 0 .Q: Tensor(\"layer_normalization0encoder_postion_wise_ff/add_1:0\", shape=(64, 1500, 512), dtype=float32) ;K_s: Tensor(\"layer_normalization0encoder_postion_wise_ff/add_1:0\", shape=(64, 1500, 512), dtype=float32)\n",
      "MultiHeadAttention.self.dropout_rate: Tensor(\"base_mode_sub_layer_multi_head_attention_encoder1/sub:0\", dtype=float32)\n",
      "scaled_dot_product_attention_batch.===============================================================>mask is not none? False\n",
      "dot_product:====================================================================================> Tensor(\"base_mode_sub_layer_multi_head_attention_encoder1/MatMul_1:0\", shape=(64, 8, 1500, 64), dtype=float32)\n",
      "self.sequence_length: 1500\n",
      "@@@========================>layer_input: Tensor(\"layer_normalization0encoder_postion_wise_ff/add_1:0\", shape=(64, 1500, 512), dtype=float32) ;layer_output: Tensor(\"base_mode_sub_layer_multi_head_attention_encoder1/dense_3/BiasAdd:0\", shape=(64, 1500, 512), dtype=float32)\n",
      "LayerNormResidualConnection.use_residual_conn: True\n",
      "layer_normalization:==================>variable_scope: layer_normalization1encoder_multi_head_attention\n",
      "output_conv1: Tensor(\"sub_layer_postion_wise_feed_forwardencoder1/conv1:0\", shape=(64, 1500, 1, 1), dtype=float32)\n",
      "@@@========================>layer_input: Tensor(\"layer_normalization1encoder_multi_head_attention/add_1:0\", shape=(64, 1500, 512), dtype=float32) ;layer_output: Tensor(\"sub_layer_postion_wise_feed_forwardencoder1/Squeeze:0\", shape=(64, 1500, 512), dtype=float32)\n",
      "LayerNormResidualConnection.use_residual_conn: True\n",
      "layer_normalization:==================>variable_scope: layer_normalization1encoder_postion_wise_ff\n",
      "encoder_fn. 1 .Q: Tensor(\"layer_normalization1encoder_postion_wise_ff/add_1:0\", shape=(64, 1500, 512), dtype=float32) ;K_s: Tensor(\"layer_normalization1encoder_postion_wise_ff/add_1:0\", shape=(64, 1500, 512), dtype=float32)\n",
      "encoder_fn.ended.Q: Tensor(\"layer_normalization1encoder_postion_wise_ff/add_1:0\", shape=(64, 1500, 512), dtype=float32) ;K_s: Tensor(\"layer_normalization1encoder_postion_wise_ff/add_1:0\", shape=(64, 1500, 512), dtype=float32) ;time spent: 0.4023168087005615\n",
      "logits (64, 19)\n",
      "labels (64, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-08-15 15:17:33,076:INFO:1epoch: f1:0.441113949265775   acc:0.4406446540880503\n",
      "2018-08-15 15:19:22,304:INFO:2epoch: f1:0.4949696738299477   acc:0.451749213836478\n",
      "2018-08-15 15:21:11,283:INFO:3epoch: f1:0.49357964035325363   acc:0.5003930817610063\n",
      "2018-08-15 15:23:00,344:INFO:4epoch: f1:0.46649731457193955   acc:0.48653694968553457\n",
      "2018-08-15 15:24:49,309:INFO:5epoch: f1:0.5073927077403181   acc:0.500687893081761\n",
      "2018-08-15 15:26:38,444:INFO:6epoch: f1:0.5709372332392986   acc:0.5771422955974843\n",
      "2018-08-15 15:28:27,480:INFO:7epoch: f1:0.5917240251242682   acc:0.597189465408805\n",
      "2018-08-15 15:30:16,865:INFO:8epoch: f1:0.601667710738157   acc:0.6132075471698113\n",
      "2018-08-15 15:32:06,023:INFO:9epoch: f1:0.5876701763486154   acc:0.5994496855345912\n",
      "2018-08-15 15:33:55,441:INFO:10epoch: f1:0.5889426481976898   acc:0.5838246855345912\n",
      "2018-08-15 15:35:44,678:INFO:11epoch: f1:0.5986425924521063   acc:0.6099646226415094\n",
      "2018-08-15 15:37:33,701:INFO:12epoch: f1:0.6259269268137746   acc:0.6193003144654088\n",
      "2018-08-15 15:39:22,568:INFO:13epoch: f1:0.6261165478884768   acc:0.6294221698113207\n",
      "2018-08-15 15:41:11,248:INFO:14epoch: f1:0.6232940788885448   acc:0.6261792452830188\n",
      "2018-08-15 15:42:59,850:INFO:15epoch: f1:0.5958768985663454   acc:0.5857900943396226\n",
      "2018-08-15 15:44:48,303:INFO:16epoch: f1:0.6340243633270323   acc:0.6347287735849056\n",
      "2018-08-15 15:46:36,998:INFO:17epoch: f1:0.5762415693908155   acc:0.5738011006289309\n",
      "2018-08-15 15:48:25,655:INFO:18epoch: f1:0.6266907798207296   acc:0.6295204402515723\n",
      "2018-08-15 15:50:14,500:INFO:19epoch: f1:0.6300382766277801   acc:0.627751572327044\n",
      "2018-08-15 15:52:03,480:INFO:20epoch: f1:0.6336563076324581   acc:0.6360062893081762\n",
      "2018-08-15 15:53:52,263:INFO:21epoch: f1:0.6336138915408985   acc:0.6294221698113207\n",
      "2018-08-15 15:55:40,879:INFO:22epoch: f1:0.6375727144973636   acc:0.6382665094339622\n",
      "2018-08-15 15:57:29,303:INFO:23epoch: f1:0.6466797703769092   acc:0.6436713836477987\n",
      "2018-08-15 15:59:17,741:INFO:24epoch: f1:0.6435863878867705   acc:0.6443592767295597\n",
      "2018-08-15 16:01:06,425:INFO:25epoch: f1:0.6324522628797192   acc:0.6293238993710691\n",
      "2018-08-15 16:02:54,986:INFO:26epoch: f1:0.6510345878079317   acc:0.64937106918239\n",
      "2018-08-15 16:04:43,657:INFO:27epoch: f1:0.6526569760907271   acc:0.6529088050314465\n",
      "2018-08-15 16:06:32,343:INFO:28epoch: f1:0.6513255151106193   acc:0.6525157232704403\n",
      "2018-08-15 16:08:21,000:INFO:29epoch: f1:0.6512728439895754   acc:0.6490762578616353\n",
      "2018-08-15 16:10:09,661:INFO:30epoch: f1:0.6531437228991626   acc:0.655562106918239\n",
      "2018-08-15 16:11:58,042:INFO:31epoch: f1:0.6550800205703758   acc:0.6553655660377359\n",
      "2018-08-15 16:13:46,517:INFO:32epoch: f1:0.6580594428320926   acc:0.6575275157232704\n",
      "2018-08-15 16:15:35,090:INFO:33epoch: f1:0.6557583536917078   acc:0.6553655660377359\n",
      "2018-08-15 16:17:23,946:INFO:34epoch: f1:0.6562178991707861   acc:0.657625786163522\n",
      "2018-08-15 16:19:12,572:INFO:35epoch: f1:0.6571965406670252   acc:0.654874213836478\n",
      "2018-08-15 16:21:01,179:INFO:36epoch: f1:0.659805062314284   acc:0.6577240566037735\n",
      "2018-08-15 16:22:49,742:INFO:37epoch: f1:0.660312534915105   acc:0.6613600628930818\n",
      "2018-08-15 16:24:38,744:INFO:38epoch: f1:0.6617328025862624   acc:0.6620479559748428\n",
      "2018-08-15 16:26:27,416:INFO:39epoch: f1:0.6651913449744471   acc:0.6655856918238994\n",
      "2018-08-15 16:28:15,914:INFO:40epoch: f1:0.6625727831733415   acc:0.664308176100629\n",
      "2018-08-15 16:30:04,820:INFO:41epoch: f1:0.6651812503680005   acc:0.6648977987421384\n",
      "2018-08-15 16:31:53,612:INFO:42epoch: f1:0.6640204776748178   acc:0.6642099056603774\n",
      "2018-08-15 16:33:42,034:INFO:43epoch: f1:0.6643354493732782   acc:0.6650943396226415\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-58fdbaecb0b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdev_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdev_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-86-9ab3bcbbc1eb>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(rain_X, train_y, dev_X, dev_y, batch_size, epoch)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mnum_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                 \u001b[0mcurrent_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0;31m#saver.save(sess,checkpoint_prefix,global_step=current_step)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-86-9ab3bcbbc1eb>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(x_batch, y_batch)\u001b[0m\n\u001b[1;32m     24\u001b[0m                 }\n\u001b[1;32m     25\u001b[0m                 _,step,loss=sess.run(\n\u001b[0;32m---> 26\u001b[0;31m                     [model.train_op,model.global_step,model.loss_val],feed_dict)\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mdev_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 feed_dict={\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(train_X,train_y,dev_X,dev_y,64,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(tf.random_normal_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
